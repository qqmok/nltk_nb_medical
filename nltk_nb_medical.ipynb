{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f6d5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\qmok9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\qmok9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\qmok9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\qmok9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c424ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = requests.Session()\n",
    "URL = \"https://en.wikipedia.org/w/api.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "420b558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories(title):\n",
    "    \"\"\"Get all categories an article belongs to.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        The title of the article\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    result : list\n",
    "        A list of categories of the article (strings)\n",
    "    \"\"\"\n",
    "    global URL, S\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"categories\",\n",
    "        \"titles\": title\n",
    "    }\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()\n",
    "    \n",
    "    PAGES = DATA[\"query\"][\"pages\"]\n",
    "    result = []\n",
    "\n",
    "    for k, v in PAGES.items():\n",
    "        for cat in v['categories']:\n",
    "            result.append(cat[\"title\"])\n",
    "    return result\n",
    "\n",
    "\n",
    "def is_medical(title):\n",
    "    \"\"\"Determine if an article with given title is a medical article\n",
    "    by searching in its categories for medical keywords.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        The title of the article\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    boolean\n",
    "        Whether the artical is medical or not\n",
    "    \"\"\"\n",
    "    categories = \" \".join(get_categories(title)).lower()\n",
    "    return \"medicine\" in categories or \"medical\" in categories\n",
    "\n",
    "\n",
    "def get_random_articles():\n",
    "    \"\"\"Get random articles from Wikipedia.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    medical : set\n",
    "        A set of medical article titles\n",
    "    non_medical : set\n",
    "        A set of non-medical article titles\n",
    "    \"\"\"\n",
    "    global URL, S\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"random\",\n",
    "        \"rnlimit\": \"max\",\n",
    "        \"rnnamespace\": \"0\"\n",
    "    }\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()\n",
    "    medical = []\n",
    "    non_medical = []\n",
    "    \n",
    "    for article in DATA[\"query\"][\"random\"]:\n",
    "        if is_medical(article[\"title\"]):\n",
    "            medical.append(article[\"title\"])\n",
    "        else:\n",
    "            non_medical.append(article[\"title\"])\n",
    "    return medical, non_medical\n",
    "\n",
    "\n",
    "def get_clean_text(title):\n",
    "    \"\"\"Get clean text from an article.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        The title of the article\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    result : list\n",
    "        Clean text without HTML tags and special characters\n",
    "    \"\"\"\n",
    "    global URL, S\n",
    "    PARAMS = {\n",
    "        \"action\": \"parse\",\n",
    "        \"format\": \"json\",\n",
    "        \"page\": title,\n",
    "        \"prop\": \"text\"\n",
    "    }\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()\n",
    "    raw_text = BeautifulSoup(DATA[\"parse\"][\"text\"][\"*\"], 'html.parser').get_text()\n",
    "    result = re.sub('[^A-Za-z]+', ' ', raw_text)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_clean_tokens(title):\n",
    "    \"\"\"Get preprocessed word tokens of an article.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        The title of the article\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    result : list\n",
    "        A list of \n",
    "    \"\"\"\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    tokenized = nltk.word_tokenize(get_clean_text(title))\n",
    "    stopwords_removed = [word for word in tokenized if not word.lower() in nltk.corpus.stopwords.words(\"english\")]\n",
    "    result = [lemmatizer.lemmatize(stemmer.stem(word)) for word in stopwords_removed]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6969858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(min_medical_articles, min_non_medical_articles):\n",
    "    \"\"\"Get random medical articles and random non-medical articles.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    min_medical_articles : int\n",
    "        Minimum number of medical articles to be fetched\n",
    "    min_non_medical_articles : int\n",
    "        Minimum number of non-medical articles to be fetched\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    result_medical : set\n",
    "        A set of medical article titles\n",
    "    result_non_medical : set\n",
    "        A set of non-medical article titles\n",
    "    \"\"\"\n",
    "    global URL, S\n",
    "    medical_articles_count = 0\n",
    "    result_medical = set()\n",
    "    result_non_medical = set()\n",
    "    \n",
    "    while len(result_medical) < min_medical_articles or len(result_non_medical) < min_non_medical_articles:\n",
    "        medical_articles, non_medical_articles = get_random_articles()\n",
    "        result_medical.update(medical_articles)\n",
    "        result_non_medical.update(non_medical_articles)\n",
    "    return result_medical, result_non_medical\n",
    "\n",
    "def prepare_bows(vocabulary, bows, data, tag, limit):\n",
    "    \"\"\"Update the current vocabulary and Bag of Words model with new text data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocabulary : set\n",
    "        A set of the complete vocabulary to be updated\n",
    "    bows : list\n",
    "        A list of Bag of Words data to be updated\n",
    "    data : list\n",
    "        A list of new articles to be added to the current vocabulary and bows\n",
    "    tag : string\n",
    "        Class of the articles in data (medical/non-medical)\n",
    "    limit: int\n",
    "        Number of articles to be added\n",
    "    \"\"\"\n",
    "    vocabulary = vocabulary\n",
    "    bows = bows\n",
    "\n",
    "    for article in data:\n",
    "        word_counts = {}\n",
    "\n",
    "        clean_tokens = get_clean_tokens(article)\n",
    "        vocabulary.update(clean_tokens)\n",
    "\n",
    "        for word in clean_tokens:\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += 1\n",
    "            else:\n",
    "                word_counts[word] = 1\n",
    "\n",
    "        bows.append((word_counts, tag))\n",
    "        if len(bows) >= limit:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea07d31",
   "metadata": {},
   "source": [
    "### Fetch Wikipedia Data and create BoWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60acfa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_medical, data_non_medical = fetch_data(50, 50)\n",
    "vocabulary = set()\n",
    "bows_medical = []\n",
    "bows_non_medical = []\n",
    "prepare_bows(vocabulary, bows_medical, data_medical, \"medical\", 50)\n",
    "prepare_bows(vocabulary, bows_non_medical, data_non_medical, \"non_medical\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e8d0d",
   "metadata": {},
   "source": [
    "### Prepare Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f70276d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "bows = bows_medical[:50]+bows_non_medical[:50]\n",
    "random.shuffle(bows)\n",
    "train_set = bows[:50]\n",
    "test_set = bows[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e7ea1",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03ae9acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy: 0.6\n",
      "Most Informative Features\n",
      "                 control = 1              medica : non_me =      3.3 : 1.0\n",
      "                 databas = 1              medica : non_me =      3.3 : 1.0\n",
      "                    link = 1              medica : non_me =      3.2 : 1.0\n",
      "                       p = 1              medica : non_me =      3.2 : 1.0\n",
      "                   place = None           medica : non_me =      3.2 : 1.0\n",
      "                    time = None           medica : non_me =      3.2 : 1.0\n",
      "                    sinc = None           non_me : medica =      3.2 : 1.0\n",
      "                wikidata = None           non_me : medica =      3.2 : 1.0\n",
      "                    turn = None           non_me : medica =      3.1 : 1.0\n",
      "                     doi = None           non_me : medica =      2.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.classify.NaiveBayesClassifier.train(train_set)\n",
    "print(\"Classifier accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
